\documentclass[runningheads,a4paper]{article}

\usepackage[utf8]{inputenc}

% \usepackage{natbib}
% \bibliographystyle{apalike-fr}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\setcounter{tocdepth}{4}
\usepackage{graphicx}

\usepackage[spanish]{babel} % Pour adopter les règles de typographie française
\usepackage[T1]{fontenc} % Pour que les lettres accentuées soient reconnues

\usepackage{url}
\usepackage{mydef}

\begin{document}

%\mainmatter 

\title{Notas para tesis}

%\titlerunning{Notas para tesis}

\author{I}

%\institute{ }

%\authorrunning{ }

%\toctitle{Résumé}
%\tocauthor{{}}

\maketitle

\section{Prior y marginal}

En estadística bayesiana se tiene que $x \sim p\left(x \,|\, \theta \right)$ %
en donde $\theta \sim \Pi\left( \theta \right)$. Si tenemos las observaciones %
$x_{1}, \ldots, x_{n} \stackrel{iid}{\sim} p\left(x \,|\, \theta \right)$, %
entonces la distribución \textit{a posteriori} está dada por:
%
\begin{equation} \label{eq:actual}
\Pi\left( \theta \,|\, x_{1},\ldots,x_{n} \right) = \frac{%
p\left(x_{1}, \ldots, x_{n} \,|\, \theta \right) \Pi\left( \theta \right)}%
{p\left( x_{1}, \ldots, x_{n} \right)}%
\propto %
\Pi\left( \theta \right) \prod_{i = 1}^{n} p\left(x_{i} \,|\, \theta \right).
\end{equation}

La distribución marginal de $x$ sin depender de $\theta$ está dada por %
%
\begin{equation} \label{eq:margpr}
p(x) = \int_{\Theta} p\left( x \,|\ \theta \right) \Pi( \theta ) d\theta,
\end{equation}
%
por lo que dadas las distribuciones anteriores se tiene que la distribución marginal posterior está dada por
%
\begin{equation} \label{eq:infer}
p\left( x \,|\, x_{1}, \ldots, x_{n} \right) = %
\int_{\Theta} p\left( x \,|\, \theta \right) %
\Pi\left( \theta \,|\, x_{1}, \ldots, x_{n} \right) d\theta.
\end{equation}

\section{Categórica-Dirichlet}

En particular, si $X \in \{1, 2, \ldots, k\}$ donde $k < \infty$ tal que %
$P\left( X = j \right) = p_{j} \geq 0$ para $j = 1, \ldots, k$ donde %
$\sum_{i = 1}^{k} p_{i} = 1$. Si suponemos que $p_{1}, \ldots, p_{k}$ son aleatorias entonces %
$\Pi\left( p_{1}, \ldots, p_{k} \right)$ tiene soporte sobre el simplex de dimensión $k-1$%
\footnote{El simplex de dimensión $m$ está definido como %
$S_{m} = \{y \in \mathbb{R}^{m} \,|\, y \geq 0,\, y^{T}e \leq 1\}$ %
con $e = (1, \ldots, 1)^{T} \in \mathbb{R}^{m}$.} %
puesto que uno de los valores está autodeterminado.
%
La distribución que usualmente se usa en este caso es la distribución Dirichlet%
\footnote{Para ahorrar espacio y facilitar la escritura usaré notación vectorial: %
$\underline{p} = \left( p_{1}, \ldots, p_{k}\right)$ y %
$\underline{\alpha} = \left( \alpha_{1}, \ldots, \alpha_{k}\right)$.}:
%
\begin{equation} \label{eq:dirich}
\Pi\left( \underline{p} \,|\, \underline{\alpha} \right) = %
\frac{\Gamma\left( \sum_{i = 1}^{k} \alpha_{i} \right)}%
{\prod_{i=1}^{k} \Gamma\left( \alpha_{i} \right)} %
\prod_{i=1}^{k} p_{i}^{\alpha_{i} - 1} %
\mathbbm{1}_{S_{k-1}}\left(\underline{p}\right).
\end{equation}
%
Con esto podemos se puede decir que $X \sim Cat\left(k, \underline{p} \right)$ donde
%
\begin{equation} \label{eq:categ}
p\left( x \,|\, \underline{p} \right) = \prod_{i = 1}^{k} p_{j}^{\left[x = j\right]} %
\mathbbm{1}_{1, \ldots, k}\left(x\right),
\end{equation}
%
donde 
%
\[
\left[ x = j \right] = \left\{ \begin{array}{lcl} %
0 & \text{si} & x \neq j \\
1 & \text{si} & x = j
\end{array}
\right.
\]
%
son los corchetes de Iverson.

Si se tienen observaciones $x_{1}, \ldots, x_{n} \stackrel{iid}{\sim} %
Cat\left( k, \underline{p} \right)$ donde $\underline{p} \sim %
Dirichlet\left( \underline{\alpha} \right)$ entonces la actualización, por %
\eqref{eq:actual}, \eqref{eq:dirich} y \eqref{eq:categ}, está dada por:
%
\begin{eqnarray*}
\Pi\left( \theta \,|\, x_{1}, \ldots, x_{n} \right) & \propto & %
\prod_{i = 1}^{k} p_{i}^{\alpha_{i} - 1} %
\prod_{j = 1}^{n} \prod_{l = 1}^{k} p_{l}^{\left[ x_{j} = l \right]} %
\mathbbm{1}_{S_{k-1}}\left( \underline{p} \right) \\
& = & \prod_{i = 1}^{k} p_{i}^{\alpha_{i} + %
\sum_{j = 1}^{n} \left[ x_{j} = i \right] - 1} %
\mathbbm{1}_{S_{k-1}}\left( \underline{p} \right),
\end{eqnarray*}
%
por lo que $\theta \,|\, x_{1}, \ldots, x_{n} \sim %
Dirichlet\left( \underline{\alpha} + \underline{n'} \right)$ en donde %
$\underline{n'} = \left(n_{1}', \ldots, n_{k}' \right)$, con %
$n_{i}' = \sum_{j = 1}^{n} \left[ x_{j} = i \right] $, es el vector en donde la entrada $i$ es el número de ocurrencias de ese caso en la muestra.

Como la distribución de la posterior sigue siendo Dirichlet, al igual que la prior, este %
modelo\footnote{Categórica-Dirichlet} se dice conjugado y de esto anterior es evidente que %
se puede generalizar al modelo \textit{Multinomial-Dirichlet}. Al caso especial de este modelo, %
cuando $k = 2$, se le conoce como \textit{Bernoulli-Beta}.

\section{Phadia}

\subsection{Revisión general}
\subsubsection{Pequeña introducción}

Aquí se enlistan de forma general algunos procesos desarrollados para ser usados como distribuciones a priori en problemas no paramétricos %
desde el punto de vista bayesiano. Más adelante se detallarán algunos de estos y se dirán sus propiedades. En el enfoque bayesiano %
la función de distribución desconocida, de donde se obtiene una muestra se considera como un parámetro. Por ello se deben construir %
distribuciones a priori en el espacio de todas las funciones de distribución, $\mathcal{F} \left( \chi \right)$, o en el espacio de %
medidas de probabilidad, $\Pi$, definido sobre $\left( \mathfrak{X}, \mathcal{A} \right)$.

Mientras que la función de distribución es el parámetro de mayor interés en análisis bayesiano no paramétrico a veces es más útil %
discutir el proceso \textit{a priori} en términos de una medida de probabilidad, $P$.\footnote{El proceso de Dirichlet está %
definido de esa forma.} %
Definir cosas \textit{a priori} es teóricamente complicado, por lo que se buscan funciones que cumplan ciertos requisitos para mejorar %
el asunto: el soporte de la función debe ser suficientemente amplio y la distribución posterior debe ser tratable analíticamente. 

\subsubsection{Métodos de construcción}

El tener un prior en $\mathcal{F}$ o $\Pi$ se puede clasificar en cuatro casos:
\begin{enumerate}
    \item El primer caso es bajo la distribución conjunta de probabilidades aleatorias.
    \item El segundo y tercer caso es bajo conceptos de independencia.
    \item El último caso se basa en un esquema de urnas de Polya.
\end{enumerate}
Los primeros tres casos se sustentan primordialmente en propiedades de la distribución Dirichlet, aunque nuevos métodos han surgido %
tomando como base la construcción de \textit{romper una rama}.

El primer método se debe a Ferguson y se desarrolla en términos de la distribución conjunta de las probabilidades de una partición %
medible en un conjunto arbitrario. El segundo método se fundamenta en la independencia de los incrementos sucesivos normalizados %
de una función de distribución $F$ definida en $\mathbb{R}$. El tercer método se basa en la propiedad \textit{tailfree} de la %
distribución Dirichlet.

\begin{defi}[Tailfree]
Una medida de probabilidad aleatoria $P$ se dice que es \textit{tailfree} si dada $\left\{ \pi_{n} \right\}$, una sucesión de %
particiones anidadas de $\mathbb{R}$, donde $\pi_{n+1}$ es un refinamiento de $\pi_{n}$, se cumple que %
$\left\{ %
P \left( B | A \right) \,|\, A \in \pi_{n} \text{ y } B \in \pi_{n+1} %
\right\}$ para $n = 1,2,\ldots$ son independientes.
\end{defi}

El cuarto método de construcción se basa en construir una sucesión de variables aleatorias intercambiables por medio de un %
esquema de urnas de Polya y luego aplicando un teorema de de Finetti.

\begin{defi}[Proceso de Polya]
Sea $\chi = \left\{ 1, 2, \ldots, k \right\}$, dada una urna con $\alpha_{i}$ bolas de un color ($i = 1, 2, \ldots, k$). %
Se saca una bola, creando una variable aleatoria $X_{1}$ en donde $P \left( X_{1} = i \right) = \frac{\alpha_{i}}{\sum_{j = 1}^{k} %
\alpha_{j}}$ y se remplaza con dos bolas del color que se sacó. En el siguiente paso se saca otra bola y ahora se tiene que $ P %
\left( X_{2} = j \,|\, X_{1} = i \right) = \frac{\alpha_{j} + \delta_{ij}}{\sum \alpha_{l} + 1} $. Se repite este proceso %
indefinidamente para obtener una sucesión de variables aleatorias intercambiables tomando valores en $\chi$.
\end{defi}

La distribución muestral de $X_{1}, X_{2}, \ldots$ converge casi seguramente a un vector $\mathbf{\theta} = \left( \theta_{1}, %
\ldots, \theta_{k} \right)$ con distribución Dirichlet con parámetros $\left(\alpha_{1}, \ldots, \alpha_{k} \right)$, y dado %
$\mathbf{\theta}$, las $X_{i}$ son independientes con $P\left( X_{i} = j \right) = \theta_{j}$. Un teorema de de Finetti%
\footnote{Investigar} asegura que hay una medida $\mu$ tal que la distribución de probabilidad conjunta marginal bajo esta %
medida es la misma para cualquier permutación de las variables. Blackwell y MacQueen generalizaron lo anterior tomando un %
número continuo de colores $\alpha$. Mostraron que la sucesión de $\frac{\alpha_{n}(\cdot)}{\alpha_{n}\left( \mathfrak{X} %
\right)}$ (donde $\alpha_{n}(\cdot) = \alpha(\cdot) + \sum_{i = 1}^{n} \delta_{i}(\cdot)$) converge a una medida discreta $P$, %
esta es el proceso de Dirichlet con parámetro $\alpha$.

La representación de una medida de probabilidad como una mezcla contable ha sido útil para el desarrollo de nuevas aplicaciones. %
Ferguson propusó una definición alternativa del proceso de Dirichlet. $\sum_{i = 1}^{\infty} p_{i}\delta_{\xi_{i}}$ en donde %
los puntos de acumulación de masa son aleatorios y provienen de la distribución $F_{0} = \frac{\alpha(\cdot)}{\alpha\left( %
\mathfrak{X} \right)}$, y los pesos $p_i$ son aleatorios con las restricciones $0 \leq p_{i} \leq 1$ y $\sum_{i = 1}^{\infty}%
p_{i} = 1$. El problema con esta representación es que Ferguson contruyó los pesos con la distribución gamma, lo cual lo hace %
poco práctico; Sethuraman ataca este problema generando los pesos por medio de variables aleatorias beta, esto renovó el interés %
en esta representación que da lugar a nuevos procesos llamados \textit{Ferguson-Sethuraman}.

Por esto el proceso Dirichlet sirve como base prior y se usa como fuente para ser generalizada. Antoniak propusó que $\alpha$ %
también fuera un parámetro, indexado por $u \sim H$, por lo que se tiene una mezcla de procesos de Dirichlet, $P \in \int %
\D\left( \alpha_{u} \right) dH(u)$. Dalal propusó que la medida $\alpha$ fuera invariante dentro de un grupo de transformaciones %
creando el \textit{Proceso de Dirichlet Invariante}. Lo, escribiendo $f(x) = \int K(x, u) dG(u)$, con $K$ un kernel conocido %
y $G \in \D(\alpha)$, pudo poner priors en un espacio de funciones de densidad. Tomando $\alpha\Pp{\X}$ como una función %
positiva en lugar de constante, Walker y Muliere generalizaron el proceso de Dirichlet para que el soporte incluya funciones %
absolutamente continuas (este proceso se llama \textit{beta-Stacy}).

\subsubsection{Procesos a priori}

El proceso de Dirichlet de Ferguson cumple con los dos requisitos básicos para un proceso a priori: es simple, está definido en %
un espacio de probabilidad arbitrario y pertenece a una familia de prioris conjugada. Lijoi y Prünster indentifican la %
conjugacidad como estructural o paramétrica; en la primera la distribución posterior tiene la misma estructura que la prior %
mientras que en la segunda la distribución posterior es igual a la prior pero con cambios en los parámetros. El proceso de %
Dirichlet tiene un parámetro interpretable; dada una muestra aleatoria $\mathbf{X} \sim P \in \D\Pp{\alpha}$, Ferguson demostró %
que la distribución posterior, dada la muestra es también un proceso de Dirichlet, en particular $P \,|\,\mathbf{X} \in \D%
\Pp{\alpha + \sum_{i = 1}^{n} \delta_{x_{i}}}$. Esta propieda hace posible la derivación de estimadores bayesianos no paramétricos %
de varias funciones de $P$ actualizando $\alpha$ En realidad $\alpha$ puede ser visto como una representación de dos parámetros: %
$F_{0}\Pp{\cdot} = \overline{\alpha}\Pp{\cdot} = \frac{\alpha\Pp{\cdot}}{\alpha\Pp{\X}}$ y $M = \alpha\Pp{\X}$. $F_{0}$ se %
interpreta como una adivinanza a priori (o distribución base) de $F$ y $M$ es el tamaño poblacionas a prior o el parámetro de %
presición (o nuestra creencia de que $F_{0}$ es correcta). La media posterior de $F$ es una combinación convexa de $F_{0}$ y la %
función de distribución empírica. 

El proceso de Dirichlet es el único proceso a priori en el que la distribución de $P\Pp{A}$ depende únicamente del número de %
observaciones dentro de $A$ y no de su ubicación y esto es considerado como una debilidad; pero el mayor problema es que el %
soporte solamente contiene medidad de probabilidad discretas.\footnote{Algunas aplicaciones recientes demuestran que este problema %
no es tan grave como podría parecer y que resulta benéfico.} El proceso de Dirichlet, a pesar de ser popular y satisfacer muchas %
demandas, no era adecuado para estimar densidades y algunos otros problemas. Para esto se crearon algunas extensiones. 

Respecto a la estimación basada en datos censurados por la derecha, asumiendo el proceso Dirichlet como prior, se sigue que %
la distribución posterior es ua mezcla de procesos de Dirichlet. Esto llevó al desarrollo de mezclas de procesos de Dirichlet. %
(Antoniak) Este proceso también tiene la propiedad de conjugacidad; sea $\mathbf{\theta} \sim P \in \int_{U} \D\Pp{\alpha_{u}} %
dH(u)$ una muestra de tamaño $n$, entonces $P \,|\, \mathbf{\theta} \in \int_{U} \D\Pp{\alpha_{u} + \sum_{i = 1}^{n} %
\delta_{\theta_{i}}} dH_{\mathbf{\theta}}(u)$. Estos procesos son útiles en problemas de evaliación biológica, pero obtener %
la expresión explícita de la distribución posterior es díficil por lo que se confía en procedimientos computacionales.

El proceso de Dirichlet es no parámetrico en el sentido de que tiene un soporte amplir; por esto Dalal vio la necesidad de %
que la prior debía de tener una estructura inherente, como simetría, o alguna propiedad de invarianza. Esto lo llevó a %
definir un proceso invariante respecto a un grupo de transformaciones medibles que selecciona una función de distribución %
invariante con probabilidad uno. Eso lo llamó \textit{proceso de Dirichlet invariante}, $\DGI\Pp{\alpha}$,con parámetro %
$\alpha$. La propiedad de conjugacidad también se cumple para este tipo de procesos.

Teh \textit{et al} proponen modelos jerárquicos donde los parámetros de las distribuciones a priori son asignados a priori %
con hiperparámetros. Un modelo general que contiene el proceso de Dirichlet, fue propuesto por Pitman, es el de \textit{modelos %
de sampleo de especies} y la probabilidad está dada por
\[
P\Pp{\cdot} = \sum_{j = 1}^{\infty} p_{j} \delta_{\xi_{j}}\Pp{\cdot} + 
            \Pp{1 - \sum_{j = 1}^{\infty} p_{j}} Q\Pp{\cdot},
\]
donde $Q$ es una medida de probabilidad correspondiente a la distribución continua $G$, $\xi_{i} \sim G$ y $\sum_{j = 1}^%
{\infty} p_{j} \leq 1$.

\subsection{Proceso de Dirichlet}

Este proceso es el más popular y usado como medida a priori en análisis no paramétrico bayesiano, este suceso se debe a su %
tratabilidad matemática; junto a sus generalizaciones, son de las priores más usadas e importantes en la modelación de %
datos covariados de grandes dimensiones. Un proceso de Dirichlet a priori con parámetro $\alpha$ para una función de %
distribución $F$ es una medida de probabilidad en el espacio de funciones de distribución y tiene dos parámetros importantes: %
una distribución base $F_{0}$ y la precisión $M$. Ferguson define el proceso de Dirichlet en términos de una medida de %
probabilidad aleatoria; desafortunadamente la concentración del proceso es en medidad de probabilidad discretas.

\subsubsection{Definición}

Sea $P$ una medida de probabilidad en $\Pp{\X, \Alg}$ en donde $\X$ es un espacio métrico separable y $\Alg = \sigma\Pp{\X}$, %
además sea $\Pi$ el conunto de todas las medidas de probabilidad en $\Pp{\X, \Alg}$. ($P$ es el parámetro y $\Pp{\Pi, %
\sigma\Pp{Pi}}$ es el esspacio parametral.) Se denota como $F$ a la función de distribución correspondiente a $P$ y %
$\mathcal{F}$ el espacio de todas las funciones de distribución. 

Sea $D\Pp{\gamma_{1}, \ldots, \gamma_{k}}$ la distribución Dirichlet $k-1$-dimensional con densidad
\[
f\Pp{x_{1},\ldots,x_{k-1}} = \frac{\Gamma\Pp{\gamma_{1} +\cdots+ \gamma_{k}}}{\Gamma\Pp{\gamma_{1}} \cdots \Gamma\Pp{\gamma_{k}}} %
                            \prod_{i = 1}^{k-1} x_{i}^{\gamma_{i} - 1} \Pp{1 - \sum_{i = 1}^{k-1} x_{i}}^{\gamma_{k} - 1},
\]
sobre $S_{k-1} = \Set{\mathbf{x} \in \R^{k-1} \,|\, \mathbf{x} \geq \mathbf{0} \text{ y } \norm[1]{\mathbf{x}} \leq 1}$  y %
$\gamma_{i} \in \R_{>0}$. 

Decimos que $P$ es una medida de probabilidad aleatoria en $\Pp{\X, \Alg}$ si para cualquier $A \in \Alg$, $P\Pp(A)$ es %
aleatoria con valores en $\Bb{0, 1}$, $P\Pp{\X} = 1$ casi seguramente y $P$ es finitamente aditiva.

\begin{defi}[Proceso de Dirichlet](Ferguson) 
Sea $\alpha$ una medida finita no negativa en $\Pp{\X, \Alg}$. Una probabilidad aleatoria $P$ es un proceso de Dirichlet %
en $\Pp{\X, \Alg}$ con parámetro $\alpha$ si para cada $k > 0$ entero y una partición medible $\Pp{A_{1}, \ldots, A_{k}}$ %
de $\X$, el vector $\Pp{P\Pp{A_{1}}, \ldots, P\Pp{A_{k}}}$ tiene la distribución $D\Pp{\alpha\Pp{A_{1}}, \ldots, \alpha %
\Pp{A_{k}}}$.
\end{defi}

Una definición alternativa está dada por una mezcla contable en la que los pesos se derivan de un proceso gamma y masas %
puntuales en puntos aleatorios. Definió el proceso Dirichlet como un proceso gamma con los incrementos divididos por su %
suma. Ishwaran y James describen una formulación alternativa para los pesos, en donde usan exponenciales con parámetro %
$1$. Otro proceso de construcción se da por Sethuraman y Tiwari en el que los pesos se derivan usando la distribución %
beta con parámetros $1$ y $\alpha\Pp{\X}$; su representación de una medida de probabilidad $P$ con prior Dirichlet $\D%
\Pp{\alpha}$ es
\[
P\Pp{A} = \sum_{j = 1}^{\infty} p_{j} \delta_{\xi_{j}} \Pp{A}, \qquad A \in \Alg,
\]
con $\xi_{j} \stackrel{iid}{\sim} \overline{\alpha}\Pp{\cdot}$ tomando valores en $\X$, $p_{1} = V_{1}$ y $p_{j} = V_{j} %
\prod_{i = 1}^{j-1} \Pp{1 - V_{i}}$ con $V_{j} \stackrel{iid}{\sim} B\Pp{1, \alpha\Pp{\X}}$. Esta construcción se llama %
construcción de romper una rama. Esta representación demuestra que el proceso de Dirichlet escoge ua distribución discreta %
con probabilidad 1.

Blackwell y MacQueen también proponen una definición alternativa. Sea $\Set{X_{n} \,|\, n \geq 1}$ una sucesión de variables %
aleatorias tomando valores en $\X$ como sigue: para cada $A \in \Alg$ sea $P\Pp{X_{1} \in A} = \frac{\alpha\Pp{A}}{\alpha\Pp{%
\X}}$ y 
\begin{equation} \label{eq:blackmac}
P\Pp{X_{n+1} \in A \,|\, X_{1}, \ldots, X_{n}} = \frac{\alpha_{n}\Pp{A}}{\alpha_{n}\Pp{\X}} %
                                               = \frac{\alpha\Pp{A} + \sum_{i = 1}^{n} \delta_{x_{i}}\Pp{A}}{\alpha\Pp{\X} + n}.
\end{equation}
A esta sucesión se le llama \textit{sucesión de Polya con parámetro $\alpha$}; Blackwell y MacQueen probaron que la sucesión %
$\frac{\alpha_{n}\Pp{\cdot}}{\alpha_{n}\Pp{\X}}$ converge a una medida discreta $P$ y $P$ es el proceso de Dirichlet con %
parámetro $\alpha$. Podemos notar que \eqref{eq:blackmac} se puede expresar como
\[
P\Pp{X_{n+1} \in A \,|\, X_{1}, \ldots, X_{n}} = \sum_{i = 1}^{n} \frac{1}{\alpha\Pp{\X} + n} \delta_{x_{i}}\Pp{A} + 
                                                \frac{\alpha\Pp{\X}}{\alpha\Pp{\X} + n} \overline{\alpha}\Pp{A}.
\]

\begin{defi}(Ferguson)
Sea $P$ una medida de probabilidad aleatoria en $\Pp{\X, \Alg}$. $X_{1}, \ldots, X_{n}$ es una muestra de $P$ si para cada $m %
> 0$ entero y conjuntos medibles $A_{1}, \ldots, A_{m}, C_{1}, \ldots, C_{n}$ de $\X$,
\[
P\Pp{X_{1} \in C_{1}, \ldots X_{n} \in C_{n} | P\Pp{A_{1}}, \ldots, P\Pp{A_{n}}, P\Pp{C_{1}}, \ldots, P\Pp{C_{n}}} %
 \!=\! \prod_{j = 1}^{n} P\Pp{C_{j}},
\]
casi seguramente.
\end{defi}

\subsubsection{Propiedades}

Para estas propiedades se asume que $P \in \D\Pp{\alpha}$ y dada $P$ $X_{1}, \ldots, X_{n}$ es una muestra de $P$. Además %
se define $M = \alpha\Pp{\X}$.
\begin{enumerate}
    \item El proceso de Dirichlet escoge una medida de probabilidad discreta con probabilidad $1$; esto es cierto aunque %
    $\alpha$ sea continua.
    \item Por la discreción del proceso de Dirichlet, la distribución predictiva de una observación futura es dada por %
    \begin{equation} \label{eq:actualpd}
    X_{n+1} \,|\, X_{1}, \ldots, X_{n} \sim \frac{M}{M + n}\overline{\alpha} + \frac{n}{M + n} \frac{1}{n} \sum_{j = 1}^{%
    K} n_{j} \delta_{X_{j}^{*}}.
    \end{equation}
    \item El siguiente teorema establece la conjugacidad del proceso de Dirichlet con respecto a observaciones no %
    censuradas:
    \begin{teor}[Ferguson]
    Sea $P \in \D\Pp{\alpha}$ y dada $P$ sea $X \sim P$ una muestra aleatoria de tamaño uno, entonces la distribución %
    marginal de $X$ es $\overline{\alpha}$; además la distribución posterior de $P$ dada $X$ es $\D\Pp{\alpha + %
    \delta_{x}}$. Si $X_{1}, \ldots, X_{n}$ es una muestra aleatoria de $P$ entonces la distribución posterior de $P$ %
    dada la muestra es $\D\Pp{\alpha + \sum_{i = 1}^{n} \delta_{x_{i}}}$.
    \end{teor}
    \item Para tomar una muestra de un proceso Dirichlet se puede realizar el procedimiento de romper una rama o la %
    extensión de la urna de Polya. La diferencia en estos métodos es la exactitud del primero contra la aproximación %
    del segundo. Su deficiencia en generar $P$ es que se necesitan infinitas repeticiones y eso no es posible, por lo %
    que se toma un criterio de paro en el tamaño. Otro método de aproximación es tomar los pesos de una distribución %
    Dirichlet simétrica con parámetro $\frac{\alpha\Pp{\X}}{N}$ y los puntos de masa se obtienen de $\overline{\alpha}$.
\end{enumerate}

\subsubsection{Mezclas y generalizaciones del proceso de Dirichlet}

Para resolver problemas de estimación bayesianos Antoniak definió mezclas de procesos Dirichlet indexando el parámetro %
$\alpha$ por $\theta \sim H\Pp{\theta}$, por lo que $P \,|\, \theta \sim \D\Pp{\alpha_{\theta}}$. Por otra parte, Lo se %
dio cuenta que al tratar con funciones de densidad el proceso de Dirichlet no era adecuado, por lo que propusó otro tipo %
de mezclas; modeló una función de densidad aleatoria en $\R$ como $f(x) = \int K(x, s) dG(s)$ en donde $K(x,s)$ es un %
kernel conocido en $\R \times \X$ y $G$ es un proceso Dirichlet. 

Ferguson consideró una mezcla contable de densidades normales formulando la función de densidad como $f(x) = \sum_{i = %
1}^{\infty} p_{i} \Nor\Pp{x \,|\, \mu_{i}, \sigma_{i}}$. Esto puede ser escrito como $f(x) = \int \Nor\Pp{x \,|\, \mu, %
\sigma} dG\Pp{\mu, \sigma}$. Para esto toma la representación de Sethuraman, definiendo los pesos $p_{j}$ con parámtro %
$M$ y $\Pp{\mu_{j}, \sigma_{j}}$ son $iid$ con respecto al prior gamma-normal; esto prueba que $G$ es un proceso Dirchlet %
con parámetro $\alpha = M G_{0}$, donde $G_{0}$ es el prior para $\Pp{\mu, \sigma}$.

Mezclas gaussianas también surgen con Escobar; sea $Y_{i} \,|\, \mu_{i} \sim \Nor\Pp{\mu_{i}, 1}$, $\mu_{i} \,|\, G %
\stackrel{iid}{\sim} G$ con $\mu_{i}$ y $G$ desconocidas. El objetivo es estimar las medias por medio de las observaci%
ones. Junto con West, también uso un modelo de mezclas gaussianas para estimación de densidades; dado $\Pp{\mu_{i}, %
\sigma_{i}^{2}}$ se tienen observaciones independientes $Y_{1}, \ldots, Y_{n}$ tales que $Y_{i} \,|\, \Pp{\mu_{i}, %
\sigma_{i}^{2}} \sim \Nor\Pp{\mu_{i}, \sigma_{i}^{2}}$ y $\nu_{i} = \Pp{\mu_{i}, \sigma_{i}^{2}}$ son muestreados de %
una distribución a priori $G$ sobre $\R \times \R_{> 0}$. Los autores asumen que $G \sim \D\Pp{M G_{0}}$ donde $G_{%
0}$ es la distribución a priori sobre $\R \times \R_{> 0}$. Por ser un proceso Dirichlet $\nu_{n+1} \,|\, \nu_{1}, %
\ldots, \nu_{n}$ seguirá una distribución de la forma \eqref{eq:actualpd}. De lo anterior proceden a derivar la %
distribución de $Y_{n+1} \,|\, \nu_{1}, \ldots, \nu_{n}$, que resulta ser una mezcla de $n$ normales y una $t$ de %
Student, con esto prubas que  $Y_{n+1} \,|\, Y_{1},\ldots, Y_{n}$ tiene la distribución predictiva $\int P\Pp{%
Y_{n+1} \,|\, \mathbf{\nu}} dP\Pp{\mathbf{\nu} \,|\, Y_{1}, \ldots, Y_{n}}$.

Un tercer tipo de mezcla conlleva los modelos jerárquicos donde los parámetros de la distribución a priori tienen %
priores asignadas con hiperparámetros. 

Por otro lado el proceso Dirichlet se prestó para muchas generalizaciones o se vio que era un caso particular de %
otros procesos. Como ejemplo de esto segundo, el proceso de Dirichlet es claramente un caso particular de el %
proceso Dirichlet invariante y de mezclas de procesos Dirichlet; si se define en $\R$ el proceso es neutral a la %
derecha, una transformación del proceso beta da como resultado el proceso Dirichlet que también resulta ser un %
caso particular de proceso beta-Stacy. Varios de los procesos relacionados al proceso Dirichlet fueron resultado %
de la representación de Sethuraman. Si la suma contable se trunca a $N < \infty$ términos, $N$ fijo o aleatorio, %
se genera una clase de distribuciones piores discretas; si los pesos definidos por $B\Pp{1, \alpha\Pp{\X}}$ se %
obtienen por una distribución beta con dos parámetros otro grupo de priores emergen. Otro grupo surge indexando %
las masas con covariables; el cuarto grupo es resultado de otro tipo de extensión, si $\delta$ se cambia por %
una medida de probabilidad $G$ no degenerada se tiene el proceso Dirichlet de kernels.

\subsubsection{Proceso Dirichlet Invariante}

Sea $\Gru = \Set{g_1, \ldots, g_{k}}$ un grupo de transformaciones medibles en un espacio $p$-dimensional euclidia%
no. U conjunto $B \in \X$ se dice $\Gru$-invariante si $B = gB$ para toda $g \in \Gru$, una medida finita no %
negativa $\gamma$ se dice $\Gru$-invariante si $\gamma\Pp{A} = \gamma\Pp{gA}$ para toda $g \in \Gru$ y todo $A %
\in \X$. Una partición medible de $\X$ se dice $\Gru$-invariante si $A_{j} = gA_{j}$ para toda $g \in \Gru$ y %
$j = 1, \ldots, m$.

\begin{defi}[Dalal]
Una medida de probabilidad aleatoria $\Gru$-invariante es un proceso Dirichlet invariante si existe una medida, %
$\alpha$, $\Gru$-invariante en $\Pp{\X, \sigma\Pp{\X}}$ tal que para cada partición medible $\Gru$-invariante %
de $\X$ la distribución conjunta de $\Pp{P\Pp{A_{1}}, \ldots, P\Pp{A_{m}}}$ es $D\Pp{\alpha\Pp{A_{1}}, \ldots, %
\alpha\Pp{A_{m}}}$. Se denota $P \in \DGI\Pp{\alpha}$.
\end{defi}

Tiwari extendió la represetación de Sethuraman al proceso Dirichlet invariante. Sea $\alpha$ una medida $\Gru$-%
invariante en $\Pp{\X, \sigma\Pp{\X}}$, sean $\Pp{p_{1}, p_{2},\ldots}$ y $\Pp{\xi_{1}, \xi_{2}, \ldots}$ dos %
sucesiones independientes de variables aleatorias $iid$, con las condiciones dadas en la representación %
contable del proceso Dirichlet, entonces la medida de probabilidad aleatoria $P$ dada por
\[
P\Pp{A} = \sum_{j = 1}^{\infty} p_{j} \frac{1}{k} \sum_{i = 1}^{k} \delta_{g_{i}, \xi_{j}}\Pp{A}, \qquad
            A \in \sigma\Pp{\X},
\]
es un proceso Dirichlet invariante con parámetro $\alpha$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Cosas
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Muestreo}

Suponiendo que $P \in \D\Pp{\alpha}$ tiene representación de Sethuraman con \textit{stick-breaking} entonces se %
puede introducir una variable latente $u$ para controlar el número de elementos que componen la distribución. Sea %
$u \sim U\Pp{0, 1}$, entonces
\[
P\Pp{\cdot \,|\, u} = \sum_{j = 1}^{\infty} w_{j} \delta_{\theta_{j}}\Pp{\cdot} \Ind\Pp{u < w_{j}}
\]
tiene un número finito de componentes si se asume que $\Set{\E\Bb{w_{i}}}_{i = 1}^{\infty}$ es una sucesión decreciente.

\begin{proof}
Por hipótesis se tiene que los pesos aleatorios cumplen $w_{j} \geq 0$ para toda $j$ y $\sum_{j = 1}^{\infty} w_{j} = 1$. %
Ahora, como $\Set{\E\Bb{w_{i}}}_{i = 1}^{\infty}$ es decreciente se puede tomar la sucesión como los pesos. Ahora basta %
probar que $\Set{w_{j}}_{j = 1}^{\infty}$ es una sucesión que converge a $0$, pues si $u \sim U\Pp{0, 1}$ solamente un %
número finito de pesos será mayor que $u$.

Sea $\varepsilon > 0$, como $\sum_{j = 1}^{\infty} w_{j} = 1$ se sigue que existe $N = N_{\varepsilon} \in \N$ tal que %
para toda $n \geq N$ entera se cumple que 
\[
\abs{\sum_{i = 1}^{n} w_{j} - 1} < \frac{\varepsilon}{2}.
\]
Luego, si $n > N$ se obtiene lo siguiente:
\[
\begin{array}{rcl}
\abs{w_{n}} & = & \abs{\sum_{j = 1}^{n} w_{j} - \sum_{i = 1}^{n-1} w_{i}} \\
 & \leq & \abs{\sum_{j = 1}^{n} w_{j}} + \abs{\sum_{i = 1}^{n-1} w_{i}} \\
 & < & \varepsilon.
\end{array}
\]
Por lo tanto, si $u \in \Pp{0, 1}$ y se define $A = \Set{j \in \N \,|\, w_{j} > u}$, entonces $\#\Pp{A} \leq N_{u} < \infty$.
\end{proof}

Es claro que el proceso original $P$ se puede recuperar de la siguiente manera.
\[
P(\cdot) = \lim_{u \to 0} P\Pp{\cdot \,|\, u}
\]

Esto se puede extender a una mezcla contable de kernels. Es decir, si a
\[
P(x) = \sum_{i = 1}^{\infty} w_{i} K(x \,|\, \theta_{i})
\]
le agregamos la variable latente $u$ entonces 

\end{document}